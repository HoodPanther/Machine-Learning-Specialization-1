# Machine Learning Specialization University of Washington
Coursera Assignment and Project of Machine learning specialization on coursera from University of washington.

### Specialization certificate: <a href="https://www.coursera.org/account/accomplishments/specialization/certificate/J44YK5UKMHGZ">https://www.coursera.org/account/accomplishments/specialization/certificate/J44YK5UKMHGZ</a>

### Installation:
* Install anaconda
* Install graphlab
* download files
* `python file.py`
* You can also run the file as ipynb for graphical results
* import your dataset in place of default dataset

### 1. Machine learning: A case study approach:

#### Certificate: <a href="https://www.coursera.org/account/accomplishments/verify/2HV93JYKF8KV">https://www.coursera.org/account/accomplishments/verify/2HV93JYKF8KV</a>

In this, I learned about the various models in machine learning and used the pre-implemented models in graphlab to get an over view.

### 2. Regression:

#### Case Study - Predicting Housing Prices

#### Certificate: <a href="https://www.coursera.org/account/accomplishments/certificate/NQS9ZND73NAK">https://www.coursera.org/account/accomplishments/certificate/NQS9ZND73NAK</a>

In our first case study, predicting house prices, I created models that predicts a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,etc).  

In this course, I explored regularized linear regression models for the task of prediction and feature selection.  I handled very large sets of features and select between models of various complexity.  I also analyzed the impact of aspects of data such as outliers on your selected models and predictions. To fit these models, I implemented optimization algorithms that scale to large datasets.

#### Work done
* Compare and contrast bias and variance when modeling data.
* Estimate model parameters using optimization algorithms.
* Tune parameters with cross validation.
* Analyze the performance of the model.
* Describe the notion of sparsity and how LASSO leads to sparse solutions.
* Deploy methods to select between models.
* Exploit the model to form predictions. 
* Build a regression model to predict prices using a housing dataset.

### 3. Classification:

#### Case Studies: Analyzing Sentiment & Loan Default Prediction

#### Data set:
 * Amazon review database consisting 183531 entries 
 * Lending club data sub-set consisting of 122607 entries
 
#### Certificate: <a href="https://www.coursera.org/account/accomplishments/certificate/PKGCUAQ7CURM">https://www.coursera.org/account/accomplishments/certificate/PKGCUAQ7CURM</a>

In this case study on analyzing sentiment, I created models that predicted a class (positive/negative sentiment) from input features (text of the reviews, user profile information, etc). In this second case study for the specialization, loan default prediction, I tackled financial data, and predicted when a loan is likely to be risky or safe for the bank. 

#### Work done
* Handle binary and multiclass classification problems.
* Implement a logistic regression model for large-scale classification.  
* Create a non-linear model using decision trees.
* Improve the performance of any model using boosting.
* Scale methods with stochastic gradient ascent.
* Describe underlying decision boundaries.  
* Build a classification model to predict sentiment in a product review dataset.  
* Analyze financial data to predict loan defaults.
* Use techniques for handling missing data.
* Evaluate models using precision-recall metrics.

### 4. Clustering:

#### Case study: Finding Similar Documents

#### Data set: Wikipedia articles subset

#### Certificate: <a href="https://www.coursera.org/account/accomplishments/certificate/QGF2HN9AUSUR">https://www.coursera.org/account/accomplishments/certificate/QGF2HN9AUSUR</a>

This is the third case study in the specialization, finding similar documents, I examined similarity-based algorithms for retrieval. In this course, I also examined structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA). Implemented expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce.

#### Work done

* Create a document retrieval system using k-nearest neighbors.
* Identify various similarity metrics for text data.
* Reduce computations in k-nearest neighbor search by using KD-trees.
* Produce approximate nearest neighbors using locality sensitive hashing.
* Cluster documents by topic using k-means.
* Examine probabilistic clustering approaches using mixtures models.
* Fit a mixture of Gaussian model using expectation maximization (EM).
* Perform mixed membership modeling using latent Dirichlet allocation (LDA).
* Describe the steps of a Gibbs sampler and how to use its output to draw inferences.
* Compare and contrast initialization techniques for non-convex optimization
